import time
import threading
from collections import defaultdict

# Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ù„Ø§Ø³ P2PNode Ø§Ø² ÙØ§ÛŒÙ„ Ù‚Ø¨Ù„ÛŒ
# (Ø¯Ø± Ù¾Ø±ÙˆÚ˜Ù‡ ÙˆØ§Ù‚Ø¹ÛŒØŒ Ø§ÛŒÙ† Ø±Ø§ Ø§Ø² Ù…Ø§Ú˜ÙˆÙ„ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ import Ú©Ù†ÛŒØ¯)

class SimpleP2PCluster:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù„Ø§Ø³ØªØ± Ø§Ø² Ú†Ù†Ø¯ Node"""
    
    def __init__(self, num_nodes=3, base_port=5000):
        self.nodes = []
        self.base_port = base_port
        
        # Ø§ÛŒØ¬Ø§Ø¯ NodeÙ‡Ø§
        for i in range(num_nodes):
            from p2p_hadoop_node import P2PNode  # ÙØ±Ø¶: ÙØ§ÛŒÙ„ Ù‚Ø¨Ù„ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
            node = P2PNode(port=base_port + i)
            self.nodes.append(node)
    
    def start_all(self):
        """Ø´Ø±ÙˆØ¹ ØªÙ…Ø§Ù… NodeÙ‡Ø§"""
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ú©Ù„Ø§Ø³ØªØ±...")
        for i, node in enumerate(self.nodes):
            node.start()
            print(f"  âœ“ Node {i+1} Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯")
            time.sleep(0.5)
        
        # Ø²Ù…Ø§Ù† Ø¨Ø±Ø§ÛŒ Ú©Ø´Ù Ù‡Ù…ØªØ§ÛŒØ§Ù†
        print("\nğŸ” Ú©Ø´Ù Ù‡Ù…ØªØ§ÛŒØ§Ù†...")
        time.sleep(3)
        
        for i, node in enumerate(self.nodes):
            print(f"  Node {i+1}: {len(node.peers)} Ù‡Ù…ØªØ§ Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
    
    def distribute_file(self, filepath):
        """ØªÙˆØ²ÛŒØ¹ ÙØ§ÛŒÙ„ Ø¯Ø± Ú©Ù„Ø§Ø³ØªØ±"""
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† Node Ø¨Ø±Ø§ÛŒ ØªÙ‚Ø³ÛŒÙ…
        print(f"\nğŸ“¤ ØªÙˆØ²ÛŒØ¹ ÙØ§ÛŒÙ„: {filepath}")
        chunk_hashes = self.nodes[0].split_and_store(content, chunk_size=200)
        
        return chunk_hashes
    
    def run_distributed_wordcount(self, chunk_hashes):
        """Ø§Ø¬Ø±Ø§ÛŒ word count Ø¨Ù‡ ØµÙˆØ±Øª ØªÙˆØ²ÛŒØ¹ Ø´Ø¯Ù‡"""
        print("\nâš™ï¸  Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙˆØ²ÛŒØ¹â€ŒØ´Ø¯Ù‡...")
        
        # Ù‡Ø± Node Ù‚Ø³Ù…ØªÛŒ Ø§Ø² chunkÙ‡Ø§ Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        results = []
        chunks_per_node = len(chunk_hashes) // len(self.nodes) + 1
        
        threads = []
        for i, node in enumerate(self.nodes):
            start_idx = i * chunks_per_node
            end_idx = min((i + 1) * chunks_per_node, len(chunk_hashes))
            node_chunks = chunk_hashes[start_idx:end_idx]
            
            if node_chunks:
                thread = threading.Thread(
                    target=lambda n, c, r: r.append(n.map_reduce(c, 'word_count')),
                    args=(node, node_chunks, results)
                )
                threads.append(thread)
                thread.start()
        
        # Ù…Ù†ØªØ¸Ø± Ø§ØªÙ…Ø§Ù… ØªÙ…Ø§Ù… threadÙ‡Ø§
        for thread in threads:
            thread.join()
        
        # Reduce Ù†Ù‡Ø§ÛŒÛŒ - ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ Ù‡Ù…Ù‡ NodeÙ‡Ø§
        print("\nğŸ”— ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬...")
        final_result = defaultdict(int)
        for result in results:
            for word, count in result.items():
                final_result[word] += count
        
        return dict(final_result)
    
    def show_cluster_stats(self):
        """Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ú©Ù„Ø§Ø³ØªØ±"""
        print("\nğŸ“Š Ø¢Ù…Ø§Ø± Ú©Ù„Ø§Ø³ØªØ±:")
        total_chunks = 0
        for i, node in enumerate(self.nodes):
            num_chunks = len(node.chunks)
            total_chunks += num_chunks
            print(f"  Node {i+1} (:{node.port}): {num_chunks} chunkØŒ {len(node.peers)} peer")
        print(f"  Ø¬Ù…Ø¹ Ú©Ù„: {total_chunks} chunk Ø¯Ø± Ú©Ù„Ø§Ø³ØªØ±")
    
    def stop_all(self):
        """ØªÙˆÙ‚Ù ØªÙ…Ø§Ù… NodeÙ‡Ø§"""
        print("\nğŸ›‘ ØªÙˆÙ‚Ù Ú©Ù„Ø§Ø³ØªØ±...")
        for node in self.nodes:
            node.stop()


def demo_wordcount():
    """Ù†Ù…Ø§ÛŒØ´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø§ ÛŒÚ© Ù…Ø«Ø§Ù„ word count"""
    
    # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© ÙØ§ÛŒÙ„ ØªØ³Øª
    test_content = """
    Ø¯Ø§Ø³ØªØ§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙˆØ²ÛŒØ¹ Ø´Ø¯Ù‡
    
    Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ…Ø±Ú©Ø² Ø­Ø§Ú©Ù… Ø¨ÙˆØ¯Ù†Ø¯
    Ø§Ù…Ø§ Ø¨Ø§ Ø±Ø´Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ ØªÙˆØ²ÛŒØ¹ Ø´Ø¯Ù‡ Ù¾ÛŒØ¯Ø§ Ø´Ø¯
    
    Ø¯Ø± Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ ØªÙˆØ²ÛŒØ¹ Ø´Ø¯Ù‡ Ù‡Ø± Ú¯Ø±Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯
    Ø§ÛŒÙ† Ø¨Ø§Ø¹Ø« Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª Ùˆ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    
    Hadoop Ùˆ Spark Ø§Ø² Ù…Ø¹Ø±ÙˆÙâ€ŒØªØ±ÛŒÙ† Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙˆØ²ÛŒØ¹ Ø´Ø¯Ù‡ Ù‡Ø³ØªÙ†Ø¯
    Ø§Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ù…Ø§ ÛŒÚ© Ù†Ø³Ø®Ù‡ Ø³Ø§Ø¯Ù‡ peer-to-peer Ø³Ø§Ø®ØªÛŒÙ…
    
    Ø¯Ø± Ù…Ø¹Ù…Ø§Ø±ÛŒ peer-to-peer Ù‡ÛŒÚ† Ù†Ù‚Ø·Ù‡ Ù…Ø±Ú©Ø²ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
    Ù‡Ù…Ù‡ Ú¯Ø±Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù‡Ù… Ø¨Ø±Ø§Ø¨Ø± Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø±Ù‚Ø±Ø§Ø± Ú©Ù†Ù†Ø¯
    """
    
    with open('test_file.txt', 'w', encoding='utf-8') as f:
        f.write(test_content)
    
    # Ø´Ø±ÙˆØ¹ Ú©Ù„Ø§Ø³ØªØ±
    cluster = SimpleP2PCluster(num_nodes=3, base_port=5000)
    cluster.start_all()
    
    # ØªÙˆØ²ÛŒØ¹ ÙØ§ÛŒÙ„
    chunk_hashes = cluster.distribute_file('test_file.txt')
    
    # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±
    cluster.show_cluster_stats()
    
    # Ø§Ø¬Ø±Ø§ÛŒ word count
    result = cluster.run_distributed_wordcount(chunk_hashes)
    
    # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
    print("\nğŸ“ˆ Ù†ØªØ§ÛŒØ¬ Ù†Ù‡Ø§ÛŒÛŒ - Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ú©Ù„Ù…Ø§Øª ØªÚ©Ø±Ø§Ø± Ø´Ø¯Ù‡:")
    sorted_words = sorted(result.items(), key=lambda x: x[1], reverse=True)
    for word, count in sorted_words[:15]:
        if len(word) > 2:  # ÙÙ‚Ø· Ú©Ù„Ù…Ø§Øª Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø±
            print(f"  {word}: {count} Ø¨Ø§Ø±")
    
    # ØªÙˆÙ‚Ù
    input("\nâ¸ï¸  Enter Ø¨Ø²Ù†ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬...")
    cluster.stop_all()


def demo_comparison():
    """Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø³Ø±Ø¹Øª Ø¨Ø§/Ø¨Ø¯ÙˆÙ† ØªÙˆØ²ÛŒØ¹"""
    import random
    import string
    
    # ØªÙˆÙ„ÛŒØ¯ ÛŒÚ© Ù…ØªÙ† Ø¨Ø²Ø±Ú¯
    def generate_text(size_kb):
        words = ['Ø¯Ø§Ø¯Ù‡', 'Ù¾Ø±Ø¯Ø§Ø²Ø´', 'ØªÙˆØ²ÛŒØ¹', 'Ø´Ø¯Ù‡', 'Ø³ÛŒØ³ØªÙ…', 'Ú©Ù„Ø§Ø³ØªØ±', 
                 'Ù…Ø­Ø§Ø³Ø¨Ø§Øª', 'Ù…ÙˆØ§Ø²ÛŒ', 'Ú¯Ø±Ù‡', 'Ø´Ø¨Ú©Ù‡']
        text = ' '.join(random.choice(words) for _ in range(size_kb * 50))
        return text
    
    text = generate_text(10)  # 10KB Ù…ØªÙ†
    
    with open('large_test.txt', 'w', encoding='utf-8') as f:
        f.write(text)
    
    print("ğŸ ØªØ³Øª Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø³Ø±Ø¹Øª")
    print("="*50)
    
    # ØªØ³Øª Ø¨Ø§ 1 Node
    print("\n1ï¸âƒ£  Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ 1 Node...")
    cluster1 = SimpleP2PCluster(num_nodes=1, base_port=5010)
    cluster1.start_all()
    
    start_time = time.time()
    chunks1 = cluster1.distribute_file('large_test.txt')
    result1 = cluster1.run_distributed_wordcount(chunks1)
    time1 = time.time() - start_time
    
    cluster1.stop_all()
    print(f"  â±ï¸  Ø²Ù…Ø§Ù†: {time1:.2f} Ø«Ø§Ù†ÛŒÙ‡")
    
    time.sleep(2)
    
    # ØªØ³Øª Ø¨Ø§ 3 Node
    print("\n3ï¸âƒ£  Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ 3 Node...")
    cluster3 = SimpleP2PCluster(num_nodes=3, base_port=5020)
    cluster3.start_all()
    
    start_time = time.time()
    chunks3 = cluster3.distribute_file('large_test.txt')
    result3 = cluster3.run_distributed_wordcount(chunks3)
    time3 = time.time() - start_time
    
    cluster3.stop_all()
    print(f"  â±ï¸  Ø²Ù…Ø§Ù†: {time3:.2f} Ø«Ø§Ù†ÛŒÙ‡")
    
    # Ù†Ù…Ø§ÛŒØ´ Ù†ØªÛŒØ¬Ù‡
    speedup = time1 / time3 if time3 > 0 else 0
    print(f"\nğŸš€ Ø§ÙØ²Ø§ÛŒØ´ Ø³Ø±Ø¹Øª: {speedup:.2f}x")


if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªÙˆØ²ÛŒØ¹ Ø´Ø¯Ù‡ P2P        â•‘
    â•‘  Decentralized Hadoop-like System   â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    print("Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:")
    print("1. Ù†Ù…Ø§ÛŒØ´ Word Count ØªÙˆØ²ÛŒØ¹ Ø´Ø¯Ù‡")
    print("2. ØªØ³Øª Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø³Ø±Ø¹Øª")
    print("3. Ù‡Ø± Ø¯Ùˆ")
    
    choice = input("\nØ§Ù†ØªØ®Ø§Ø¨ (1/2/3): ").strip()
    
    if choice == '1':
        demo_wordcount()
    elif choice == '2':
        demo_comparison()
    elif choice == '3':
        demo_wordcount()
        print("\n" + "="*50 + "\n")
        demo_comparison()
    else:
        print("Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø§Ù…Ø¹ØªØ¨Ø±!")